# Apache Spark Learning Journey

This repository documents my learning process and experiments with **Apache Spark**, focusing on understanding distributed data processing, Spark APIs, and best practices in building scalable applications.

---

## Summary of Learning
- Hands-on experience in creating and configuring Spark applications.  
- Simulated a multi-core cluster locally using `local[3]` configuration.  
- Explored and practiced with **Spark DataFrame APIs** and their transformations/actions.  
- Implemented **Unit Testing** for Spark jobs to ensure correctness and reliability.  
- Learned the basics of Spark’s **logging and debugging** using Log4J.  

---

## Tech Stack
1. **Apache Spark** – Core distributed data processing framework  
2. **Python (PySpark)** – Primary programming language with supporting packages  
3. **Log4J** – Application logging and monitoring in Spark  
